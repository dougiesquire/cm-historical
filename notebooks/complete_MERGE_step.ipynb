{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2519980c-ab56-4e0f-aa72-090ff92e5750",
   "metadata": {},
   "source": [
    "# Complete partially-finished MERGE step of a historical run\n",
    "The 48 hour walltime limit on Gadi means that it's not possible to finish the MERGE step in a single job for long (> ~60 year) large ensemble runs. I need a better fix in the long run, but for now this notebook continues each MERGE job where it left off\n",
    "\n",
    "Here I use `region` to fill the incomplete block and then pick up continue writing with `append`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c477570f-3865-45d6-9384-a17ffedd30ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import glob\n",
    "import zarrtools\n",
    "import numpy as np\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4564a032-6953-4bd4-a72d-f0a3b2a19a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_jobqueue import PBSCluster\n",
    "\n",
    "walltime = '48:00:00'\n",
    "cores = 48\n",
    "memory = '192GB'\n",
    "cluster = PBSCluster(processes=1,\n",
    "                     walltime=str(walltime), cores=cores, memory=str(memory),\n",
    "                     job_extra=['-l ncpus='+str(cores),\n",
    "                                '-l mem='+str(memory),\n",
    "                                '-P xv83',\n",
    "                                '-l jobfs=100GB',\n",
    "                                '-l storage=gdata/xv83+gdata/v14+scratch/v14'],\n",
    "                     local_directory='$PBS_JOBFS',\n",
    "                     # env_extra=['export MALLOC_TRIM_THRESHOLD_=\"0\"'],\n",
    "                     header_skip=[\"select\"])\n",
    "\n",
    "cluster.scale(jobs=1)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "raw",
   "id": "99f7fa44-f337-41f8-9c5d-426180a87c10",
   "metadata": {},
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "user = str(os.environ['USER'])\n",
    "local_directory = '/scratch/ux06/'+user+'/dask-worker-space/'\n",
    "cluster = LocalCluster(n_workers=1,\n",
    "                       dashboard_address=':8768',\n",
    "                       local_directory=local_directory)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7847245b-4d11-45b4-9aaa-dcbef14847cd",
   "metadata": {},
   "source": [
    "### Change this depending on what `realm` your doing and when it died"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13715e0b-0c70-467a-8ce0-8e597d20e278",
   "metadata": {},
   "outputs": [],
   "source": [
    "realm = 'atmos_isobaric_daily'\n",
    "incomplete_block = slice(23716, 24024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc52ebf1-59dc-43ae-a110-5ae77a972255",
   "metadata": {
    "tags": []
   },
   "source": [
    "### How does the partially-written data look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21013afd-3920-4efe-8400-3f4737964ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = xr.open_zarr(f'/g/data/xv83/ds0092/CAFE/historical/WIP/c5-d60-pX-hist-19601101-96mem/ZARR/{realm}.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78087861-076f-4192-93af-2526ecbe9456",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['temp'].isel(time=incomplete_block).isnull().sum().values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2f2318-becb-469a-bbb0-82fa81abbbe1",
   "metadata": {},
   "source": [
    "### Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc3ec4d-0fe2-4e1d-bf68-6dceeef11de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_to_merge = sorted(glob.glob('/g/data/xv83/ds0092/CAFE/historical/WIP/c5-d60-pX-hist-19601101-96mem/ZARR/mem???'))\n",
    "zarr_path = '/g/data/xv83/ds0092/CAFE/historical/WIP/c5-d60-pX-hist-19601101-96mem/ZARR/'\n",
    "config_file = '/g/data/xv83/ds0092/CAFE/historical/WIP/c5-d60-pX-hist-19601101-96mem/mem004/zarr_specs_CAFE-f6.json'\n",
    "number_of_time_blocks = 100\n",
    "\n",
    "with open(config_file) as json_file:\n",
    "    specs = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf4ce61-f37e-4b15-a940-39f26689d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "zarrify_kwargs = specs[realm].copy()\n",
    "if 'load_chunks' in zarrify_kwargs:\n",
    "    zarrify_kwargs.pop('load_chunks')\n",
    "zarr_name = zarrify_kwargs.pop('zarr_name')\n",
    "zarrify_kwargs['output_file'] = os.path.join(zarr_path, zarr_name)\n",
    "done_file = os.path.join(zarr_path, f'{zarr_name}{os.path.extsep}done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49271553-fb3d-41e0-b496-3fbbfc617157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import zarr\n",
    "\n",
    "print(f'Merging {zarr_name}...', flush=True)\n",
    "_open = dask.delayed(xr.open_zarr)\n",
    "datasets = []\n",
    "merged_files = []\n",
    "for path in paths_to_merge:\n",
    "    input_file = os.path.join(path, zarr_name)\n",
    "    input_zip_file = os.path.join(path, zarr_name+os.path.extsep+'zip')\n",
    "    # Preferably use DirectoryStore if available, else ZipStore\n",
    "    if os.path.exists(input_file):\n",
    "        datasets.append(_open(input_file, consolidated=True))\n",
    "        merged_files.append(input_file)\n",
    "    elif os.path.exists(input_zip_file):\n",
    "        datasets.append(_open(zarr.ZipStore(input_zip_file), consolidated=True))\n",
    "        merged_files.append(input_zip_file)\n",
    "    else:\n",
    "        raise FileNotFoundError(f'Cannot find {input_file} or {input_zip_file}')\n",
    "dataset = xr.concat(dask.compute(datasets)[0], dim='ensemble').sortby('ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db04e3-184d-4f0c-93ef-d8144f4643e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in dataset:\n",
    "    dataset[var].encoding = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa7026b-3a49-4ff0-ba9b-a99438b630d0",
   "metadata": {},
   "source": [
    "### Use `region` to write incomplete block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397f5047-840c-4562-a2df-b776fbe11f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _drop_vars(ds, sequence_dim='time'):\n",
    "    # writing a region means that all the variables MUST have sequence_dim\n",
    "    to_drop = [v for v in ds.variables if sequence_dim not in ds[v].dims]\n",
    "    return ds.drop_vars(to_drop)\n",
    "                      \n",
    "print(f'Writing {incomplete_block}...', flush=True)\n",
    "dataset_write = dataset.isel({'time': incomplete_block})\n",
    "zarrtools.zarrify(_drop_vars(dataset_write),\n",
    "                  **zarrify_kwargs,\n",
    "                  region={'time': incomplete_block},\n",
    "                  compute=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdb121e-7247-47ad-875d-b6dea90ec8d4",
   "metadata": {},
   "source": [
    "### Complete rest of collection using `append`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fda936-beca-4598-aa52-4504b4011525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _blocks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "        \n",
    "# Loop over time blocks \n",
    "BLOCK_DIM = 'time'\n",
    "time_chunk_size = zarrify_kwargs['save_chunks'][BLOCK_DIM]\n",
    "N = dataset.sizes[BLOCK_DIM]\n",
    "block_size = time_chunk_size * math.ceil(N / number_of_time_blocks / time_chunk_size) # round to nearest multiple of time chunks\n",
    "\n",
    "for idx, block in enumerate(_blocks(range(incomplete_block.stop, N), block_size)):\n",
    "    print(f'Writing {block} of {N}...', flush=True)\n",
    "    dataset_write = dataset.isel({BLOCK_DIM: slice(block.start, block.stop)})\n",
    "    zarrtools.zarrify(dataset_write,\n",
    "                      **zarrify_kwargs,\n",
    "                      append_dim=BLOCK_DIM,\n",
    "                      compute=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
